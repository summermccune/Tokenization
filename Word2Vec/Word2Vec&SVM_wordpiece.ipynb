{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1AmfvmBzuOXdvRmDHM6GvMR5w39wFW5VX",
      "authorship_tag": "ABX9TyNOH8OQkIjY5DLHB9/ewDbA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/summermccune/Tokenization/blob/main/Word2Vec/Word2Vec%26SVM_wordpiece.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wordpiece implementation based on huggingface\n",
        "https://huggingface.co/learn/nlp-course/chapter6/6\n",
        "\n",
        "This is the beginning of a wordpiece implementation, not sure if it'll be useful yet"
      ],
      "metadata": {
        "id": "xUX6hxNem4xg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "ehaoj8RkS8XF"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import glob\n",
        "from sklearn.svm import SVC\n",
        "from transformers import AutoTokenizer\n",
        "from collections import defaultdict\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_files(malwarefolder, malwareType):\n",
        "  for sample in malwarefolder:\n",
        "    with open(sample, 'r') as f:\n",
        "      data = f.read()\n",
        "      data = data.replace('\\n', ' ')\n",
        "    #append sample to opcodes list in df\n",
        "    df.loc[len(df)] = [data, malwareType]"
      ],
      "metadata": {
        "id": "m4VYX1UeL6qb"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from hugging face implementation\n",
        "def compute_pair_scores(splits):\n",
        "    letter_freqs = defaultdict(int)\n",
        "    pair_freqs = defaultdict(int)\n",
        "    for word, freq in word_freqs.items():\n",
        "        split = splits[word]\n",
        "        if len(split) == 1:\n",
        "            letter_freqs[split[0]] += freq\n",
        "            continue\n",
        "        for i in range(len(split) - 1):\n",
        "            pair = (split[i], split[i + 1])\n",
        "            letter_freqs[split[i]] += freq\n",
        "            pair_freqs[pair] += freq\n",
        "        letter_freqs[split[-1]] += freq\n",
        "\n",
        "    scores = {\n",
        "        pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])\n",
        "        for pair, freq in pair_freqs.items()\n",
        "    }\n",
        "    return scores"
      ],
      "metadata": {
        "id": "psD6DaVnpBIT"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from huggingface implementation\n",
        "def merge_pair(a, b, splits):\n",
        "    for word in word_freqs:\n",
        "        split = splits[word]\n",
        "        if len(split) == 1:\n",
        "            continue\n",
        "        i = 0\n",
        "        while i < len(split) - 1:\n",
        "            if split[i] == a and split[i + 1] == b:\n",
        "                merge = a + b[2:] if b.startswith(\"##\") else a + b\n",
        "                split = split[:i] + [merge] + split[i + 2 :]\n",
        "            else:\n",
        "                i += 1\n",
        "        splits[word] = split\n",
        "    return splits"
      ],
      "metadata": {
        "id": "NPbI9QkupPDo"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from huggnigface implementation\n",
        "def encode_word(word):\n",
        "    tokens = []\n",
        "    while len(word) > 0:\n",
        "        i = len(word)\n",
        "        while i > 0 and word[:i] not in vocab:\n",
        "            i -= 1\n",
        "        if i == 0:\n",
        "            return [\"[UNK]\"]\n",
        "        tokens.append(word[:i])\n",
        "        word = word[i:]\n",
        "        if len(word) > 0:\n",
        "            word = f\"##{word}\"\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "MlkSpAV7rCCd"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from huggingface implementation\n",
        "def tokenize(text):\n",
        "    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
        "    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
        "    encoded_words = [encode_word(word) for word in pre_tokenized_text]\n",
        "    return sum(encoded_words, [])"
      ],
      "metadata": {
        "id": "Fk_HZlCFrNOX"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating dataframe for samples and label\n",
        "df = pd.DataFrame(columns = ['opcodes','label'])\n",
        "\n",
        "#specify paths for each malware\n",
        "winwebsec = glob.glob(\"/content/drive/MyDrive/Data/malware2/winwebsec/*.txt\")\n",
        "zbot = glob.glob(\"/content/drive/MyDrive/Data/malware2/zbot/*.txt\")\n",
        "zeroaccess = glob.glob(\"/content/drive/MyDrive/Data/malware2/zeroaccess/*.txt\")\n",
        "\n",
        "#read files\n",
        "read_files(winwebsec, 0)\n",
        "read_files(zbot, 1)\n",
        "read_files(zeroaccess, 2)\n",
        "\n",
        "#huggingface wordpiece implementation\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "word_freqs = defaultdict(int)\n",
        "for text in df['opcodes']:\n",
        "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
        "    new_words = [word for word, offset in words_with_offsets]\n",
        "    for word in new_words:\n",
        "        word_freqs[word] += 1\n",
        "\n",
        "alphabet = []\n",
        "for word in word_freqs.keys():\n",
        "    if word[0] not in alphabet:\n",
        "        alphabet.append(word[0])\n",
        "    for letter in word[1:]:\n",
        "        if f\"##{letter}\" not in alphabet:\n",
        "            alphabet.append(f\"##{letter}\")\n",
        "\n",
        "alphabet.sort()\n",
        "\n",
        "vocab = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"] + alphabet.copy()\n",
        "\n",
        "splits = {\n",
        "    word: [c if i == 0 else f\"##{c}\" for i, c in enumerate(word)]\n",
        "    for word in word_freqs.keys()\n",
        "}\n",
        "\n",
        "pair_scores = compute_pair_scores(splits)\n",
        "for i, key in enumerate(pair_scores.keys()):\n",
        "    print(f\"{key}: {pair_scores[key]}\")\n",
        "    if i >= 5:\n",
        "        break\n",
        "\n",
        "best_pair = \"\"\n",
        "max_score = None\n",
        "for pair, score in pair_scores.items():\n",
        "    if max_score is None or max_score < score:\n",
        "        best_pair = pair\n",
        "        max_score = score\n",
        "\n",
        "vocab_size = 70 #could change this variable\n",
        "while len(vocab) < vocab_size:\n",
        "    scores = compute_pair_scores(splits)\n",
        "    best_pair, max_score = \"\", None\n",
        "    for pair, score in scores.items():\n",
        "        if max_score is None or max_score < score:\n",
        "            best_pair = pair\n",
        "            max_score = score\n",
        "    splits = merge_pair(*best_pair, splits)\n",
        "    new_token = (\n",
        "        best_pair[0] + best_pair[1][2:]\n",
        "        if best_pair[1].startswith(\"##\")\n",
        "        else best_pair[0] + best_pair[1]\n",
        "    )\n",
        "    vocab.append(new_token)\n",
        "\n",
        "for sample in df['opcodes']:\n",
        "  tokenize(sample)\n",
        "\n",
        "print(df.shape)\n",
        "df.head()\n",
        "\n",
        "print(df['opcodes'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QNq5mWV7KsM",
        "outputId": "6636fea4-90a0-481d-a675-91773717caae"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('p', '##u'): 0.0001024121520538698\n",
            "('##u', '##s'): 0.0001108883467590628\n",
            "('##s', '##h'): 0.0001311373252402504\n",
            "('m', '##o'): 4.65068638221382e-05\n",
            "('##o', '##v'): 4.575866625249369e-05\n",
            "('s', '##u'): 6.90163308218509e-05\n",
            "(15, 2)\n",
            "0     push mov sub push push mov mov mov mov mov mov...\n",
            "1     push mov sub mov mov call mov pop retn push mo...\n",
            "2     push mov sub xor mov push add mov mov cmp jnz ...\n",
            "3     push mov sub push push push mov mov mov mov mo...\n",
            "4     mov push mov sub lea add mov push call push pu...\n",
            "5     push mov call mov push call add mov pop retn p...\n",
            "6     push mov sub mov mov mov mov mov mov push lea ...\n",
            "7     sub push push mov push push push push call cal...\n",
            "8     push mov call mov push call add mov pop retn p...\n",
            "9     sbb jz adc inc jmp and add jz add pushf bound ...\n",
            "10    push mov sub and mov xor mov mov sub inc push ...\n",
            "11    push pop push mov or add sub retn xchg or push...\n",
            "12    push mov sub or push adc push dec xor not inc ...\n",
            "13    push mov sub xor mov xor mov mov or mov mov jm...\n",
            "14    push mov sub add mov cmp mov not jmp mov not c...\n",
            "Name: opcodes, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vlAs53atou3_"
      },
      "execution_count": 69,
      "outputs": []
    }
  ]
}