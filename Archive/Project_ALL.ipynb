{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1AQznbjfG3fmzl3edxmS6Ukp10MotSlmJ",
      "authorship_tag": "ABX9TyOp9aEZyPsQsVo7pkUGSonv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/summermccune/Tokenization-Testing-for-Malware-Data/blob/main/Project_ALL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "yLQijNUTzlDb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install hmmlearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQ_ac3yz8PVn",
        "outputId": "9cec4aea-bd26-4e31-9893-31369894cf08"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.20.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.6.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: hmmlearn in /usr/local/lib/python3.10/dist-packages (0.3.2)\n",
            "Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.10/dist-packages (from hmmlearn) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn!=0.22.0,>=0.16 in /usr/local/lib/python3.10/dist-packages (from hmmlearn) (1.2.2)\n",
            "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.10/dist-packages (from hmmlearn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVFa-yPWonL5",
        "outputId": "7ff49c35-4b3c-4d43-b364-7754c4cda456"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "from datasets import load_dataset\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers import Tokenizer, ByteLevelBPETokenizer, SentencePieceBPETokenizer\n",
        "from tokenizers.models import BPE, WordPiece, Unigram\n",
        "from tokenizers.trainers import BpeTrainer, WordPieceTrainer, UnigramTrainer\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "from gensim.models import Word2Vec\n",
        "from hmmlearn import hmm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay,accuracy_score, f1_score, precision_score, recall_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "frnlY8lV75Tb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#read in the data\n",
        "FakeRean = pd.read_csv('/content/drive/MyDrive/Tokenization-Final/Families/FakeRean.csv')\n",
        "OnLineGames = pd.read_csv('/content/drive/MyDrive/Tokenization-Final/Families/OnLineGames.csv')\n",
        "Vobfus = pd.read_csv('/content/drive/MyDrive/Tokenization-Final/Families/Vobfus.csv')\n",
        "Winwebsec = pd.read_csv('/content/drive/MyDrive/Tokenization-Final/Families/Winwebsec.csv')\n",
        "BHO = pd.read_csv('/content/drive/MyDrive/Tokenization-Final/Families/BHO.csv')\n",
        "CeeInject = pd.read_csv('/content/drive/MyDrive/Tokenization-Final/Families/CeeInject.csv')\n",
        "Renos = pd.read_csv('/content/drive/MyDrive/Tokenization-Final/Families/Renos.csv')\n",
        "\n",
        "#make csvs to df with train and test columns\n",
        "dataset = pd.concat([FakeRean, OnLineGames, Vobfus, Winwebsec, BHO, CeeInject, Renos], ignore_index=True)\n",
        "\n",
        "#drop first column\n",
        "dataset = dataset.iloc[:, 1:]\n",
        "\n",
        "print(dataset.head())\n",
        "print(dataset.tail())"
      ],
      "metadata": {
        "id": "VeNcCp4976qE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "hUDJVhh5zpqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count(df, c):\n",
        "  for row in df['Opcodes']:\n",
        "     data = row.split()\n",
        "     c.update(data)"
      ],
      "metadata": {
        "id": "s5IIqdGCzroD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def removeNonVocab(vocab, series):\n",
        "  rows = []\n",
        "  vocab_str = '|'.join(vocab)\n",
        "  pattern = '\\\\b((?!\\\\b( |' + vocab_str + ')\\\\b).)*\\\\b'\n",
        "  for row in series:\n",
        "    row = re.sub(pattern, '', row)\n",
        "    row = re.sub(' +', ' ', row)\n",
        "    rows.append(row.split())\n",
        "  return rows"
      ],
      "metadata": {
        "id": "iujvgVbZ1Pks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def opcodes_to_numbers_dict(df):\n",
        "  #creating a list of number representation for each opcode that is in the dataset\n",
        "  opcode_to_number = {}\n",
        "  count = 0\n",
        "  for opcode in df['Opcodes']:\n",
        "    opcode_to_number[opcode] = count\n",
        "    count += 1\n",
        "  print(opcode_to_number)\n",
        "\n",
        "  return opcode_to_number"
      ],
      "metadata": {
        "id": "CW-W7J7Q-nB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def opcodes_to_numbers(columns):\n",
        "  opcode_to_number = opcodes_to_numbers_dict(dataset)\n",
        "  opcode_sequences = []\n",
        "  for sample in columns:\n",
        "    temp = []\n",
        "    for opcode in sample:\n",
        "      temp.append(opcode_to_number[opcode])\n",
        "    opcode_sequences.append(temp)\n",
        "  return opcode_sequences"
      ],
      "metadata": {
        "id": "1m77_tvY1S_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_hmm_models(opcodes,n_states,n_restarts):\n",
        "  hmm_models = []\n",
        "  for opcode_seq in opcodes:\n",
        "\n",
        "      model = hmm.CategoricalHMM(n_components=n_states, n_iter=100)\n",
        "      opcode_seq = np.array(opcode_seq)\n",
        "      model.fit(opcode_seq.reshape(-1, 1))\n",
        "      hmm_models.append(model)\n",
        "      best_model = None\n",
        "      best_score = -np.inf\n",
        "\n",
        "      for i in range(n_restarts):\n",
        "        model = hmm.MultinomialHMM(n_components=n_states, n_iter=100)\n",
        "        opcode_seq = np.array(opcode_seq)\n",
        "        model.fit(opcode_seq.reshape(-1, 1))\n",
        "\n",
        "        #check if the model has a higher score than the current best model\n",
        "        score = model.score(opcode_seq.reshape(-1, 1))\n",
        "        if score > best_score:\n",
        "          best_model = model\n",
        "          best_score = score\n",
        "\n",
        "      hmm_models.append(best_model)\n",
        "\n",
        "  return hmm_models"
      ],
      "metadata": {
        "id": "kBu4p7J016Hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def b_matrix_to_features(hmm_models, max_feature_length):\n",
        "  hmm2vec_features = []\n",
        "  for model in hmm_models:\n",
        "    #determine the hidden state that has the highest probability with respect to the mov opcode\n",
        "    mov_index = np.argmax(model.emissionprob_[:, opcode_to_number['mov']])\n",
        "\n",
        "    #deem this to be the first half of the HMM2Vec feature vector, with the other row of the B matrix being the second half of the vector\n",
        "    sorted_indices = [mov_index, 1 - mov_index]\n",
        "    sorted_bmatrices = model.emissionprob_[sorted_indices]\n",
        "\n",
        "    # Flatten the rearranged B matrix to create HMM2Vec feature vector\n",
        "    feature_vector = sorted_bmatrices.flatten()\n",
        "\n",
        "    # pad or truncate feature_vector to ensure consistent length\n",
        "    if len(feature_vector) < max_feature_length:\n",
        "      feature_vector = np.pad(feature_vector, (0, max_feature_length - len(feature_vector)), mode='constant')\n",
        "    elif len(feature_vector) > max_feature_length:\n",
        "      feature_vector = feature_vector[:max_feature_length]\n",
        "\n",
        "    hmm2vec_features.append(feature_vector)\n",
        "\n",
        "  return hmm2vec_features"
      ],
      "metadata": {
        "id": "ludF1qoZ2fL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_iterator(data):\n",
        "  for i in range(0, len(dataset), batch_size):\n",
        "      yield dataset['Opcodes'][i : i + batch_size]"
      ],
      "metadata": {
        "id": "5UYhlBIu4VW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_tokenizer_trainer(alg):\n",
        "    \"\"\"\n",
        "    Prepares the tokenizer and trainer with unknown & special tokens\n",
        "    \"\"\"\n",
        "    if alg == 'BPE':\n",
        "        tokenizer = Tokenizer(BPE(unk_token = unk_token))\n",
        "        trainer = BpeTrainer(special_tokens = spl_tokens, vocab_size=v_size)\n",
        "    elif alg == 'UNI':\n",
        "        tokenizer = Tokenizer(Unigram())\n",
        "        trainer = UnigramTrainer(unk_token= unk_token, special_tokens = spl_tokens, vocab_size=v_size)\n",
        "    elif alg == 'WPC':\n",
        "        tokenizer = Tokenizer(WordPiece(unk_token = unk_token))\n",
        "        trainer = WordPieceTrainer(special_tokens = spl_tokens, vocab_size=v_size)\n",
        "\n",
        "    tokenizer.pre_tokenizer = Whitespace()\n",
        "    return tokenizer, trainer"
      ],
      "metadata": {
        "id": "3LLMty2t4cIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_tokenizer(alg):\n",
        "    \"\"\"\n",
        "    Trains the tokenizer\n",
        "    \"\"\"\n",
        "    if (alg == 'BPE' or alg == 'UNI' or alg == 'WPC'):\n",
        "      tokenizer, trainer = prepare_tokenizer_trainer(alg)\n",
        "      tokenizer.train_from_iterator(batch_iterator('train'), trainer=trainer)\n",
        "      return tokenizer\n",
        "    elif alg == 'BBPE':\n",
        "      tokenizer = ByteLevelBPETokenizer()\n",
        "      tokenizer.train_from_iterator(batch_iterator('train'),vocab_size=v_size)\n",
        "    elif alg == 'SPC':\n",
        "      tokenizer = SentencePieceBPETokenizer()\n",
        "      tokenizer.train_from_iterator(batch_iterator('train'),vocab_size=v_size)\n",
        "    tokenizer.save(\"Tokenizers/\"+alg+\"-trained.json\")\n",
        "    return tokenizer"
      ],
      "metadata": {
        "id": "TJrgTLeG4fNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(tokenizer):\n",
        "  encode = tokenizer.encode_batch(dataset['Opcodes'])\n",
        "  tokens = [encoding.tokens for encoding in encode]\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "8zzEYLU-5I_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "dYJW-BZszsk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#set up values\n",
        "batch_size = 1000\n",
        "unk_token = \"<UNK>\"\n",
        "spl_tokens = [\"<UNK>\", \"<SEP>\", \"<MASK>\", \"<CLS>\"]\n",
        "v_size = 100"
      ],
      "metadata": {
        "id": "jHafjcl6BeIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### BPE Tokenizer\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ow0csF3E6d-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BPE_tokenizer = train_tokenizer('BPE')\n",
        "BPE_tokenizer.save(\"drive/MyDrive/Tokenization-Final/Tokenizers/BPE-trained.json\")"
      ],
      "metadata": {
        "id": "SpqfdL8Yzv9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BPE_tokens = encode(BPE_tokenizer)\n",
        "print(BPE_tokens[0])"
      ],
      "metadata": {
        "id": "cgrmH1BJ7GfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding"
      ],
      "metadata": {
        "id": "I5Wf3MQlzwsI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zYMliQixz1g0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification"
      ],
      "metadata": {
        "id": "1CCn_JQRz1-O"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vhyeIiFdz4_h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
